jobs: 500
local-cores: 1
cluster: "sbatch --parsable --account={resources.account} --partition={resources.partition} --time={resources.time_min} --mem={resources.mem_mb} -c {resources.cpus} -o logs/slurm/{rule}_{wildcards} -e logs/slurm/{rule}_{wildcards}"
cluster-cancel: "scancel"
use-conda: true
conda-frontend: mamba
use-singularity: true
default-resources: [cpus=1, mem_mb=8000, time_min=120, account=gdick1, partition=standard]
resources: [cpus=500, mem_mb=3500000]
latency-wait: 60
cluster-status: "slurm-status.py"
max-status-checks-per-second: 1
singularity-args: "--home /home/$USER:/home/$USER"