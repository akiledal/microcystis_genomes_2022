---
title: "pre-process"
author: "Anders Kiledal"
date: "10/13/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = here::here())
```

This document lays out the processing of shotgun metagenomic data from a concrete sample (S3 fallen) collected from an ASR affected bridge in Northern New Jersey. A negative control sample was also submitted for sequencing, but it failed to generate a signal.


Merge paired end reads
```{r}
library(tidyverse)

system("wsl ~/miniconda2/bin/activate mpa; which bbmerge.sh")

bbmerge_path <- "/home/eandersk/miniconda2/envs/mpa/bin/bbmerge.sh"

system(paste("wsl",bbmerge_path,"--version"))

if ((grep("Windows", osVersion, ignore.case = TRUE))) { #Run on windows subsystem for linux if OS is windows
  bbmerge_path <- paste("wsl",bbmerge_path)
}

fqs <- list.files("data/qc_sequence_files/",pattern = "*.fq.gz")

samples <- fqs %>% str_remove("_S[0-9]_R[0-9]_.*") %>% unique()

files <- data.frame(sample = samples) %>% 
  mutate(forward = paste0("data/qc_sequence_files/",sample,"_S1_R1_001_val_1.fq.gz"),
         reverse = paste0("data/qc_sequence_files/",sample,"_S1_R2_001_val_2.fq.gz"))

for (i in 1:length(files)){
  system(paste0(bbmerge_path,
                " in1=", files$forward[i],
                " in2=", files$reverse[i], 
                " out=", files$sample[i], ".fastq.gz" ,
                " outu1=", files$sample[i], "_F_unmerged.fastq.gz", 
                " outu2=", files$sample[i], "_R_unmerged.fastq.gz"))
}

```

Run metaphlan
```{bash}
source ~/miniconda2/bin/activate mpa

mkdir data/metaphlan

metaphlan data/qc_sequence_files/S3_Fallen_S1_R1_001_val_1.fq,data/qc_sequence_files/S3_Fallen_S1_R2_001_val_2.fq \
  --bowtie2db ~/metaphlan_db \
  --bowtie2out data/metaphlan/metagenome.bowtie2.bz2 \
  --nproc 4 \
  --input_type fastq \
  -o data/metaphlan/S3_fallen_metaphlan.txt

```


```{bash}
source ~/miniconda2/bin/activate mpa

merge_metaphlan_tables.py data/metaphlan/S3_fallen_metaphlan.txt > data/metaphlan/table.tsv

```


## Megan

https://github.com/uhkniazi/HPRU_Metagenomics
```{bash}

cd /work/akiledal/diamond

wget ftp://ftp.ncbi.nlm.nih.gov/blast/db/FASTA/nr.gz

gunzip nr.gz ; mv nr nr.fasta

diamond makedb --in nr.fa -d nr 
```


Run diamond
```{bash}
date
ssh akiledal@biomix.dbi.udel.edu "sbatch" <<'ENDSSH'
#!/bin/bash
#SBATCH --job-name=diamond_blastx
#SBATCH --chdir=/work/akiledal/diamond
#SBATCH --mem=800G
#SBATCH -c 48
#SBATCH --mail-type=ALL
#SBATCH --mail-user=akiledal@udel.edu
#SBATCH --time=20-0

date
hostname
start=`date +%s`

###Script starts here###

mkdir temp

samples="S3_Fallen_S1_R1_001_val_1 S3_Fallen_S1_R2_001_val_2"

for s in ${samples}
do
    diamond blastx -p 48 -d nr -q ${s}.fastq -a BM_${s} -t temp
done

rm temp

###Script ends here###
echo "Duration: $((($(date +%s)-$start)/60)) minutes"
ENDSSH
```
Use diamond to convert .daa fiels to .m8 files
  --Update 10/20--this is apparently the old way of doing things, and it is now preferred to use the .daa files directly with the "meganize" funtion, and then open the .daa files with MEGAN.
```{bash}
date
ssh akiledal@biomix.dbi.udel.edu "sbatch" <<'ENDSSH'
#!/bin/bash
#SBATCH --job-name=diamond_m8
#SBATCH --chdir=/work/akiledal/diamond
#SBATCH --mem=800G
#SBATCH -c 48
#SBATCH --mail-type=ALL
#SBATCH --mail-user=akiledal@udel.edu
#SBATCH --time=20-0

date
hostname
start=`date +%s`

###Script starts here###

samples="BM_S3_Fallen_S1_R1_001_val_1 BM_S3_Fallen_S1_R2_001_val_2"

for s in ${samples}
do
    diamond view -a ${s}.daa -o ${s}.m8
done

###Script ends here###
echo "Duration: $((($(date +%s)-$start)/60)) minutes"
ENDSSH
```



Plot metacodeR of results
```{r}
library(biomformat)
library(tidyverse)
library(qiime2R)
library(readr)
library(metacoder)
library(viridis)

test <- read_tsv("data/megan/BM_S3_Fallen_S1_R1_001_val_1-ex.txt",col_names = F) %>% 
  rename(percent_abund = "X2") %>% 
  mutate(tax = row_number()) %>% 
  separate(col = X1,sep = ";",into = c("Root","Cellular","Kingdom","Phylum","Class","Order","Family","Genus","Species","Strain","A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P")) %>% 
  pivot_longer(-c(percent_abund,tax),names_to = "tax_levels",values_to = "taxa") %>% 
  mutate(new_tax_label = paste0(tax_levels,"__",taxa)) %>% 
  filter(!is.na(taxa),taxa != "", taxa != "NA") %>% 
  select(-taxa) %>% 
  spread("tax_levels","new_tax_label",fill = NA) %>% 
  unite(lineage, c("Root","Cellular","Kingdom","Phylum","Class","Order","Family","Genus","Species","Strain","A","B","C","D","E","F","G","H","I","J","K","L","M","N","O","P"),sep = ";",na.rm = TRUE) %>% select(-tax) %>% 
  rename(S3_fallen = "percent_abund") %>% 
  #filter(str_detect(lineage,"Bacteria")) %>% 
  mutate(lineage = str_remove(lineage,"Root__NCBI;Cellular__cellular organisms;"))
  #mutate(lineage = str_remove(lineage,"Root__NCBI;Cellular__Viruses;"))


met_samples <- data.frame(sample_id = "S3_fallen", Type = "Concrete")

#Make the MetaCodeR obj
  ##Greengenes class_regex = "^(.+)__(.*)$"
  ##SILVA class_regex = "^D_(.+)__(.*)$"  


obj <- parse_tax_data(test, class_cols = "lineage", class_sep = ";",
                      class_regex = "^(.+)__(.*)$",
                      class_key = c("tax_rank" = "taxon_rank", "name" = "taxon_name"))


## Calculate relative abundance
obj$data$rel_abd <- calc_obs_props(obj, "tax_data", other_cols = T)

#summing per-taxon counts
obj$data$tax_abund <- calc_taxon_abund(obj, "rel_abd",
                                       cols = met_samples$sample_id,
                                       groups = met_samples$Type)

#To get in final relative abundance form
obj$data$tax_abund$Concrete <- obj$data$tax_abund$Concrete / obj$data$tax_abund$Concrete[1]

#Make the heat tree
obj %>%
taxa::filter_taxa(Concrete > 0.0001, taxon_ranks == "Species", supertaxa = TRUE) %>%
heat_tree(node_label = taxon_names,
          node_size = Concrete,
          node_size_trans = "area",
          node_size_range = c(0.002,0.05),
          node_label_size_range = c(.015,.025),
          node_size_axis_label = "OTU count",
          initial_layout = "reingold-tilford", 
          layout = "davidson-harel",
          #layout = "automatic",
          overlap_avoidance = 5,
          repel_force = 10,
          node_label_max = 50 ,
          node_color = Concrete,
          node_color_range = c("gray80","gray80","gray80"),
          node_color_axis_label = "Relative abundance") +
          ggsave("data/shi7_out/shogun_output/MEGAN_metacodeR.pdf", device = cairo_pdf, units = "in", height = 12, width = 12, dpi = 300) +
          ggsave("data/shi7_out/shogun_output/MEGAN_metacodeR.png", type = "cairo", units = "in", dpi = 300, height = 12, width = 12)
```




## Shogun

### Pre-process with Shi7
```{bash}
date
ssh akiledal@biomix.dbi.udel.edu "sbatch" <<'ENDSSH'
#!/bin/bash
#SBATCH --job-name=shi7
#SBATCH --chdir=/work/akiledal/metagenomics_trial/
#SBATCH --mem=800G
#SBATCH -c 48
#SBATCH --mail-type=ALL
#SBATCH --mail-user=akiledal@udel.edu
#SBATCH --time=20-0

source ~/.bashrc

date
hostname
start=`date +%s`

###Script starts here###

conda activate shi7

mkdir shi7_out/input_files/

cp S3_Fallen/*.fastq.gz shi7_out/input_files/

gunzip shi7_out/input_files/*.fastq.gz

shi7 -i shi7_out/input_files/ -o shi7_out --adaptor Nextera --flash False --filter_length 50 -m 0

rm -rf shi7_out/input_files/

###Script ends here###
echo "Duration: $((($(date +%s)-$start)/60)) minutes"
ENDSSH
```





Despite claims to the contrary online, it seems to not handle fastq.gz, so files were manually gunzipped beforehand.

```{bash}
shi7 -i for_shi7/ -o shi7_out --adaptor Nextera --flash False --filter_length 50 -m 0
```



### Run the Shogun pipeline using bowtie2
```{bash}
shogun pipeline -i combined_seqs.fna -d /work/akiledal/shogun/ -o shogun_output -a bowtie2
```


### Run the Shogun pipeline using burst
```{bash}
shogun pipeline -i combined_seqs.fna -d /work/akiledal/shogun/ -o shogun_output_burst -a burst
```

### Run the Shogun BURST alignment

```{bash}
date
ssh akiledal@biomix.dbi.udel.edu "sbatch" <<'ENDSSH'
#!/bin/bash
#SBATCH --job-name=BURST_align
#SBATCH --chdir=/work/akiledal/metagenomics_trial/shi7_out
#SBATCH --mem=800G
#SBATCH -c 48
#SBATCH --mail-type=ALL
#SBATCH --mail-user=akiledal@udel.edu
#SBATCH --time=20-0

source ~/.bashrc

date
hostname
start=`date +%s`

###Script starts here###

conda activate qiime2-2020.2

shogun align -a burst -i combined_seqs.fna -d /work/akiledal/shogun/ -o shogun_burst_align -t 20

###Script ends here###
echo "Duration: $((($(date +%s)-$start)/60)) minutes"
ENDSSH
```
### Assign taxonomy
```{bash}
date
ssh akiledal@biomix.dbi.udel.edu "sbatch" <<'ENDSSH'
#!/bin/bash
#SBATCH --job-name=shogun_burst_tax
#SBATCH --chdir=/work/akiledal/metagenomics_trial/shi7_out
#SBATCH --mem=800G
#SBATCH -c 48
#SBATCH --mail-type=ALL
#SBATCH --mail-user=akiledal@udel.edu
#SBATCH --time=20-0

source ~/.bashrc

date
hostname
start=`date +%s`

###Script starts here###

conda activate qiime2-2020.2

shogun assign_taxonomy -a burst -i shogun_burst_align/alignment.burst.b6 -d /work/akiledal/shogun/ -o shogun_burst_tax

shogun normalize -i shogun_burst_tax -o shogun_burst_norm_tax

shogun coverage -i shogun_burst_align/alignment.burst.b6 -d /work/akiledal/shogun/ -o shogun_burst_coverage.txt

shogun functional -i shogun_burst_tax -d /work/akiledal/shogun/ -o shogun_burst_function

###Script ends here###
echo "Duration: $((($(date +%s)-$start)/60)) minutes"
ENDSSH
```


### Interpret results

#### BowTie2 metacodeR figure

Plot metacodeR of the BowTie2 Shogun results
```{r}
library(biomformat)
library(tidyverse)
library(qiime2R)
library(readr)
library(metacoder)
library(viridis)

table <- read_tsv("data/shogun/bt2/taxatable.strain.ra.txt") %>% 
  #mutate(S3_fallen = (S3Fallen.S1.R1.R1 + S3Fallen.S1.R1.R2) / 2 ) %>% 
  rename(S3_fallen= "S3") %>% 
  select(lineage = "#OTU ID",S3_fallen) %>% 
  write_tsv("data/shogun/bt2/rel_abund.txt") #write the table w/ F & R summarized into one sample

met_samples <- data.frame(sample_id = "S3_fallen", Type = "Concrete")


#Make the MetaCodeR obj
  ##Greengenes class_regex = "^(.+)__(.*)$"
  ##SILVA class_regex = "^D_(.+)__(.*)$"  

obj <- parse_tax_data(table, class_cols = "lineage", class_sep = ";",
                      class_regex = "^(.+)__(.*)$",
                      class_key = c("tax_rank" = "taxon_rank", "name" = "taxon_name"))


## Calculate relative abundance
obj$data$rel_abd <- calc_obs_props(obj, "tax_data", other_cols = T)

#summing per-taxon counts
obj$data$tax_abund <- calc_taxon_abund(obj, "rel_abd",
                                       cols = met_samples$sample_id,
                                       groups = met_samples$Type)

#To get in final relative abundance form
obj$data$tax_abund$Concrete <- obj$data$tax_abund$Concrete / obj$data$tax_abund$Concrete[1]

#Make the heat tree
obj %>%
taxa::filter_taxa(Concrete > 0.01, taxon_ranks == "s", supertaxa = TRUE) %>%
heat_tree(node_label = taxon_names,
          node_size = Concrete,
          node_size_trans = "area",
          node_size_range = c(0.002,0.05),
          node_label_size_range = c(.015,.025),
          node_size_axis_label = "OTU count",
          initial_layout = "reingold-tilford", 
          layout = "davidson-harel",
          #layout = "automatic",
          overlap_avoidance = 5,
          repel_force = 10,
          node_label_max = 50 ,
          node_color = Concrete,
          node_color_range = c("gray80","gray80","gray80"),
          node_color_axis_label = "Relative abundance") +
          ggsave("results/figures/shogun_bt2_community.pdf", device = cairo_pdf, units = "in", height = 12, width = 12, dpi = 300) +
          ggsave("results/figures/shogun_bt2_community.png", type = "cairo", units = "in", dpi = 300, height = 12, width = 12)
```



Plot metacodeR of the BURST Shogun results
```{r}
library(biomformat)
library(tidyverse)
library(qiime2R)
library(readr)
library(metacoder)
library(viridis)

table <- read_tsv("data/shogun/burst/taxatable.strain.ra.txt") %>% 
  #mutate(S3_fallen = (S3.Fallen.S1.R1.001.R1 + S3.Fallen.S1.R1.001.R2) / 2 ) %>% 
  mutate(S3_fallen = (S3_Fallen_R1 + S3_Fallen_R2) / 2 ) %>% 
  select(lineage = "#OTU ID",S3_fallen) %>% 
  write_tsv("data/shogun/burst/rel_abund.txt") #write the table w/ F & R summarized into one sample

met_samples <- data.frame(sample_id = "S3_fallen", Type = "Concrete")


#Make the MetaCodeR obj
  ##Greengenes class_regex = "^(.+)__(.*)$"
  ##SILVA class_regex = "^D_(.+)__(.*)$"  

obj <- parse_tax_data(table, class_cols = "lineage", class_sep = ";",
                      class_regex = "^(.+)__(.*)$",
                      class_key = c("tax_rank" = "taxon_rank", "name" = "taxon_name"))


## Calculate relative abundance
obj$data$rel_abd <- calc_obs_props(obj, "tax_data", other_cols = T)

#summing per-taxon counts
obj$data$tax_abund <- calc_taxon_abund(obj, "rel_abd",
                                       cols = met_samples$sample_id,
                                       groups = met_samples$Type)

#To get in final relative abundance form
obj$data$tax_abund$Concrete <- obj$data$tax_abund$Concrete / obj$data$tax_abund$Concrete[1]

#Make the heat tree
obj %>%
taxa::filter_taxa(Concrete > 0.01, taxon_ranks == "s", supertaxa = TRUE) %>%
heat_tree(node_label = taxon_names,
          node_size = Concrete,
          node_size_trans = "area",
          node_size_range = c(0.002,0.05),
          node_label_size_range = c(.015,.025),
          node_size_axis_label = "OTU count",
          initial_layout = "reingold-tilford", 
          layout = "davidson-harel",
          #layout = "automatic",
          overlap_avoidance = 5,
          repel_force = 10,
          node_label_max = 50 ,
          node_color = Concrete,
          node_color_range = c("gray80","gray80","gray80"),
          node_color_axis_label = "Relative abundance") +
          ggsave("results/figures/shogun_burst_community.pdf", device = cairo_pdf, units = "in", height = 12, width = 12, dpi = 300) +
          ggsave("results/figures/shogun_burst_community.png", type = "cairo", units = "in", dpi = 300, height = 12, width = 12)
```




### Kegg results

Download Kegg reference file and format it for importing (based on ANVIO tutorial)
```{bash}

wget 'https://www.genome.jp/kegg-bin/download_htext?htext=ko00001&format=htext&filedir=' -O data/reference/ko00001.keg

kegfile="data/reference/ko00001.keg"

while read -r prefix content
do
    case "$prefix" in A) col1="$content";; \
                      B) col2="$content" ;; \
                      C) col3="$content";; \
                      D) echo -e "$col1\t$col2\t$col3\t$content";;
    esac 
done < <(sed '/^[#!+]/d;s/<[^>]*>//g;s/^./& /' < "$kegfile") > data/reference/KO_Orthology_ko00001.txt

```


```{r}
library(tidyverse)
library(ggalluvial)
library(KEGGREST)


kegg_db <- read_tsv("data/reference/KO_Orthology_ko00001.txt",col_names = c("type","specific_type","pathway","gene")) %>% 
  separate(gene,c("kegg_id","kegg_name"),sep = "  ") %>% 
  separate(pathway,c("path_id","path_name"),sep = " ",extra = "merge") %>% 
  mutate(across(c(type,specific_type),~str_remove(.,"[0-9]* ")))

kegg_mod_db <- read_tsv("data/reference/KEGG_modules_info.tsv",col_names = c("mod_id","module"))

kegg_path_db <- read_tsv("data/reference/KEGG_pathways_info.tsv",col_names = c("path_id","pathway"))


kegg_master_db <- read_tsv("data/reference/ko00001.keg")


kegg_mod <- read_tsv("data/shi7_out/shogun_output/taxatable.strain.kegg.modules.txt") %>% 
  mutate(S3_fallen = S3Fallen.S1.R1.R1 + S3Fallen.S1.R1.R2,
         ra_S3_fallen = S3_fallen / sum(S3_fallen)) %>% 
  select(mod_id = "#MODULE ID", S3_fallen, ra_S3_fallen) %>% 
  left_join(kegg_mod_db)
  

kegg_path <- read_tsv("data/shi7_out/shogun_output/taxatable.strain.kegg.pathways.txt") %>% 
   mutate(S3_fallen = S3Fallen.S1.R1.R1 + S3Fallen.S1.R1.R2,
         ra_S3_fallen = S3_fallen / sum(S3_fallen)) %>% 
  select(path_id = "#PATHWAY ID", S3_fallen, ra_S3_fallen) %>% 
  left_join(kegg_path_db)

kegg <- read_tsv("data/shi7_out/shogun_output/taxatable.strain.kegg.txt") %>% 
  mutate(S3_fallen = S3Fallen.S1.R1.R1 + S3Fallen.S1.R1.R2,
         ra_S3_fallen = S3_fallen / sum(S3_fallen)) %>% 
  select(kegg_id = "#KEGG ID", S3_fallen, ra_S3_fallen) %>% 
  left_join(kegg_db)


is_alluvia_form(kegg)


kegg_alluvial <- kegg %>% select(ra_S3_fallen,type, specific_type) %>% 
  filter(!type %in% c("Brite Hierarchies","Not Included in Pathway or Brite","NA")) %>% 
  arrange(ra_S3_fallen)

order <- kegg_alluvial %>% 
  group_by(specific_type) %>% 
  summarise(ra_S3_fallen = sum(ra_S3_fallen)) %>% 
  arrange(ra_S3_fallen) %>% 
  pull(specific_type) %>% unique()

# 
# (test <- kegg_alluvial %>% 
#   ggplot(aes(y = S3_fallen, axis1 = type, axis2 = factor(specific_type,levels = order))) +
#   geom_alluvium(aes(fill = specific_type), width = 1/12) +
#   geom_stratum(aes(fill = type),width = 1/12, color = "grey") +
#   geom_label(stat = "stratum", aes(label = after_stat(stratum))) +
#   scale_x_discrete(limits = c("type", "specific_type"), expand = c(.05, .05)) +
#   #scale_fill_viridis_d() +
#   theme(legend.position = "none")
#   
#   )


kegg_alluvial %>% 
  filter(!is.na(specific_type)) %>% 
  ggplot(aes(factor(specific_type,order),ra_S3_fallen, fill = type)) +
  geom_bar(stat = "identity") +
  #facet_grid(~type,scales = "free_x") +
  coord_flip() +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  scale_fill_viridis_d() +
  labs(x = NULL, y = "relative abundance", title = "KEGG processes in concrete metagenome", subtitle = "SHOGUN results from BowTie2 alignment") +
  ggsave("results/figures/kegg_shogun_bt2.png",type = "cairo", width = 8, height = 8)


```



## Assemble with metaspades
```{bash}
date
ssh akiledal@biomix.dbi.udel.edu "sbatch" <<'ENDSSH'
#!/bin/bash
#SBATCH --job-name=metaspades
#SBATCH --chdir=/work/akiledal/metagenomics_trial/
#SBATCH --mem=800G
#SBATCH -c 8
#SBATCH --mail-type=ALL
#SBATCH --mail-user=akiledal@udel.edu
#SBATCH --time=20-0

source ~/.bashrc

date
hostname
start=`date +%s`

###Script starts here###

conda activate spades

metaspades.py \
  -t 8 \
  -m 800 \
  -1 data/qc_sequence_files/S3_fallen_val_1.fq.gz \
  -2 data/qc_sequence_files/S3_fallen_val_2.fq.gz \
  -o assembly_metaspades/S3_fallen


###Script ends here###
echo "Duration: $((($(date +%s)-$start)/60)) minutes"
ENDSSH
```




```{r}
library(Biostrings)
library(tidyverse)

metaspades_contigs <- readDNAStringSet("data/assembly_metaspades/S3_Fallen/contigs.fasta")

megahit_contigs <- readDNAStringSet("data/assembly_megahit/S3_Fallen/final.contigs.fa")

megahit_df <- as.data.frame(megahit_contigs) %>% 
  rownames_to_column("header") %>% 
  rename(sequence = "x") %>%
  separate("header",into=c("name","flag","coverage","length"),sep = " ") %>% 
  mutate(across(c("flag","coverage","length"),~as.numeric(str_remove(.,"^[a-z]*="))),
         assembler = "megahit")

metaspades_df <- as.data.frame(contigs) %>% 
  rename(sequence = "x") %>%
  rownames_to_column("contig") %>% 
  separate("contig",into = as.character(1:6),sep = "_") %>% 
  select(name = "2",
         length = "4",
         coverage = "6",
         sequence) %>% 
  mutate(across(c(length,coverage),~as.numeric(.)),
         assembler = "metaspades")

contigs_df <- full_join(metaspades_df,megahit_df)

contigs_df %>% ggplot(aes(length,fill=assembler)) +
  geom_histogram() +
  scale_y_log10() +
  labs(title = "Contig length distribution (log)") +
  theme_bw()

contigs_df %>% ggplot(aes(length,coverage,color =assembler)) +
  geom_point(alpha =0.5) +
  scale_y_log10() +
  labs(title ="Contig length vs. coverage",
       x = "length (bp)",
       y = "coverage [log]") +
  theme_bw() +
  ggsave("results/figures/assembly_summary.png",device = "png",width = 6, height = 4)

```



Assembly summary
```{r}
library(Biostrings)
library(tidyverse)

metaspades_contigs <- readDNAStringSet("data/combined_assembly_metaspades/NJ8/contigs.fasta")

megahit_contigs <- readDNAStringSet("data/combined_assembly_megahit/NJ8/final.contigs.fa")

megahit_df <- as.data.frame(megahit_contigs) %>% 
  rownames_to_column("header") %>% 
  rename(sequence = "x") %>%
  separate("header",into=c("name","flag","coverage","length"),sep = " ") %>% 
  mutate(across(c("flag","coverage","length"),~as.numeric(str_remove(.,"^[a-z]*="))),
         assembler = "megahit")

metaspades_df <- as.data.frame(metaspades_contigs) %>% 
  rename(sequence = "x") %>%
  rownames_to_column("contig") %>% 
  separate("contig",into = as.character(1:6),sep = "_") %>% 
  select(name = "2",
         length = "4",
         coverage = "6",
         sequence) %>% 
  mutate(across(c(length,coverage),~as.numeric(.)),
         assembler = "metaspades")

contigs_df <- full_join(metaspades_df,megahit_df)

contigs_df %>% ggplot(aes(length,fill=assembler)) +
  geom_histogram() +
  scale_y_log10() +
  labs(title = "Contig length distribution (log)") +
  theme_bw()

contigs_df %>% ggplot(aes(length,coverage,color =assembler)) +
  geom_point(alpha =0.5) +
  scale_y_log10() +
  scale_x_log10() +
  labs(title ="Contig length vs. coverage",
       x = "length (bp)",
       y = "coverage [log]") +
  theme_bw() +
  ggsave("results/NJ8_combined_assembly_summary.png",type = "cairo",width = 6, height = 4)

```










#Check taxonomy with SourMash
Compute signature for our genome
```{bash engine.opts='-i'}
conda activate sourmash

#sourmash compute --scaled 1000 -k 31 data/qc_sequence_files/S3_Fallen_R1.fastq.gz -o data/sourmash/S3_f.sig

#sourmash search data/sourmash/S3_f.sig ../concrete_isolate_genomes/reference/sourmash/genbank-k31.sbt.zip -n 20

sourmash lca summarize --db ../concrete_isolate_genomes/reference/sourmash/genbank-k31.lca.json.gz --query data/sourmash/S3_f.sig

```






